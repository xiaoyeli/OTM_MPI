% Simulation speed is crucial when traffic simulation is used in real-time traffic management or traffic forecasting applications \cite{lee2002framework}.
\IEEEPARstart{R}{ecent} years have seen significant technological changes in transportation with the advent of shared-mobility services such as Uber and Lyft, electric vehicles, and vehicle automation. There is a need for government agencies and industry to understand the impact of wide-spread adoption of these new technologies on the transportation system. This in turn requires regional scale modeling tools, which consume large computing and memory resources. Parallel computation and modern supercomputers provide the power and memory to handle such large-scale traffic simulations. They also enable running thousands of parallel simulations that can answer future what-if scenarios.

There are two types of approaches to parallel computation: shared memory and distributed memory.
On a shared-memory computer, parallel simulation takes advantage of the computer's many processors to execute tasks in parallel. In this case, the simulation speed-up is limited by the number of processors on one computer. On a multiprocessor distributed-memory computer system, parallel simulation can use processors on multiple computers while communicating over a message passing system. This paper describes such an approach for a fluid-based traffic simulation model. We begin by reviewing previous efforts, most of which have focused on vehicle-based (microscopic and mesoscopic) models. 

\subsection{Related Works}

\begin{table*}[htbp]
\centering
\begin{tabular} { | p{20mm} | p{20mm} | p{125mm}| } 
	\hline
	\hline
	\textbf{Software} & \textbf{Traffic Model} & \textbf{Parallel Method}\\ \hline
	Paramics \cite{cameron1996paramics}, FastTrans \cite{thulasidasan2009accelerating}    & Microscopic & Parallel computation on distributed-memory computer with MPI for inter-core communication \\ \hline
	TRANSIMS \cite{robertson1969transyt}     & Microscopic & Parallel computation on distributed-memory computer with MPI and PVM for inter-core communication \\ \hline
	AIMSUN \cite{ferrer1993aimsun2}, SEM-Sim \cite{aydt2013multi},  MEgaffic \cite{osogami2012research}     & Microscopic & Multi-threading \\ \hline
	SUMO \cite{behrisch2011sumo}    & Microscopic & Multi-threading for routing \\ \hline
	Dynemo \cite{nokel2002parallel}  & Mesoscopic & Network distribution through master-slave parallelization \\ \hline
	Mobiliti \cite{chan2018mobiliti}     & Mesoscopic & Parallel discrete event with GASNet \\ \hline
	POLARIS \cite{auld2016polaris}     & Mesoscopic & Multi-threading \\ \hline
	BEAM \cite{aboutBeam}    & Mesoscopic & Akka library \\ \hline
	\cite{xu2014mesoscopic,song2017supporting}, \cite{strippgen2009multi}     & Mesoscopic & Parallel computation on GPUs \\ \hline
	\cite{chronopoulos1998real}     & Macroscopic & Distribution of freeway sections on multiple processes of a nCubes2 distributed-memory parallel computer \\ \hline
	\cite{johnston1999parallelization}     & Macroscopic & Distribution of highway segments on multiple processes of a Cray T3W distributed-memory parallel computer \\ \hline
	OTM-MPI \cite{otmmpi}  & Macroscopic & Parallel computing on modern mult-node computer systems with MPI for inter-core communication\\ \hline
%	OTM     & Macroscopic Model & Domain decomposition of network with highway, arterial and local roads. Use MPI for inter-process communication \\ \hline
\end{tabular}
\caption{Traffic Simulation Software}
\label{tab:softwares}
\end{table*}

Since Greenshield presented the first traffic model in 1934 \cite{greenshields1934photographic}, three main model categories have emerged: microscopic models, macroscopic models, and mesoscopic models. While microscopic models represent the behavior and interaction of individual vehicles,  macroscopic models describe traffic as a continuum. Mesoscopic models are an intermediate class in which macroscopic parameters such as link capacity are applied to individual vehicles. Traffic simulation software tools usually implement a single microscopic, macroscopic or mesoscopic model. Open Traffic Models is one of a few  \textit{hybrid} simulation tools, which allow the combination of two or more models. 

Most of the traffic simulators with parallel computation capabilities are microscopic simulators. An example of this is Paramics \cite{cameron1996paramics}, which was implemented on a the T3D parallel computer with Message Passing Interface (MPI) for inter-processor communication. FastTrans \cite{thulasidasan2009accelerating} and TRANSIMS \cite{nagel2001parallel} are also microscopic simulators and use parallel computation on distributed-memory computer systems by dividing the road network into multiple subnetworks. FastTrans uses MPI to share boundary information among processes, while TRANSIMS can support both MPI and Parallel Virtual Machine (PVM) communication. AIMSUN \cite{ferrer1993aimsun2}, SEMSim \cite{aydt2013multi}, and IBM's Mega MEgaffic simulator \cite{osogami2012research} implement parallel computation with a multi-threading approach, which enables them to update multiple agents simultaneously. The popular microscopic simulator SUMO
exploits multi-threading parallel computation for vehicle routing, but the rest of simulation is single-core \cite{behrisch2011sumo}.
The BEAM simulator \cite{aboutBeam} extends MATSIM \cite{horni2016multi}, which is a microscopic agent-based traffic simulator. BEAM incorporates parallel computation via the Akka \cite{akka} library, which implements the Actor Model paradigm \cite{actorModel}. 

In the realm of mesoscopic traffic simulation, N{\"o}kel and Schmidt present a parallel implementation for Dynemo, that uses a master-slave parallelization. Mobiliti \cite{chan2018mobiliti}, a mesoscopic simulator, models links as agents and vehicles as events. It uses GASNet \cite{gasnet} to enable parallel computation on high performance computing resources. POLARIS  \cite{auld2016polaris} exploits parallel computation via multi-threading. Additional works on mesoscopic traffic simulation with parallel computation on graphics processing units include \cite{xu2014mesoscopic,song2017supporting,strippgen2009multi}. 

On the other hand, parallel computation has not been applied extensively to macroscopic traffic simulation. \cite{chronopoulos1998real} presents a parallel continuum traffic model on an nCubes2 hypercube distributed-memory parallel computer, where a freeway is partitioned into equal segments and assigned to different processors. Similarly, \cite{johnston1999parallelization} describes a parallel lax-momentum traffic model that divided the freeway into equal segments assigned to different processors of a Cray T3E distributed-memory computer. Although  \cite{chronopoulos1998real} and \cite{johnston1999parallelization} use parallel computation for macroscopic traffic simulation on distributed-memory computers, they are focused on freeways only. In addition, the reported studies involve only about 20 miles of freeway, and were executed on older computer systems.

The work presented here is based on the Open Traffic Models simulator. OTM is a full-featured traffic simulator that has in the past been used in single-process environments. OTM \cite{Gomes2019OTM} is a \textit{hybrid} simulator, in the sense that it incorporates multiple (macroscopic, mesoscopic, and microscopic) models which can be combined arbitrarily on a single network. It also supports various road geometries (turn pockets, managed lanes on freeways, etc.), multiple vehicle types, and a variety of traffic control systems (traffic signals, variable speed limits, on-board information systems, etc.). Here we focus on the parallelization of the macroscopic model only. This is both because there are many high quality distributed implementations of vehicle-based models, and because it avoid some of the complexities of asynchronous updates that occur in vehicle-based model. OTM-MPI represents, to the best of our knowledge, the first open-source, distributed-memory, macroscopic simulation model developed for modern high performance parallel machines and large networks  \cite{otmmpi}.